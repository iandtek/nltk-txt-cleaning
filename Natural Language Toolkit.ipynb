{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Text data using Natural Language Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the libraries needed, for more information of how to install Natural Language ToolKit, please check\n",
    "http://www.nltk.org/install.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd #for dataframe imports and data management\n",
    "import nltk #Natural Language ToolKit\n",
    "import string #we are going to use some strings methods in the cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the sample dataset provided from the Creative Destruction Lab as a Dataframe from a CSV File."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('applicant_ventures_without_cohort_1_overfit_test_doubled.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the data that we are going to normalize and clean from the CSV. In this example I'm going to use the \"Team Relationships\" Column of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Although there aren't co-founders in strict se...\n",
       "1      Nick and Cathy have known each other since ear...\n",
       "2                                 I am the only founder.\n",
       "3      The founder Denis and Peter have known each ot...\n",
       "4      The founders are siblings and have known each ...\n",
       "5      Ruby joined the team as the HR Consultant in A...\n",
       "6      The concept for Ardle started with my previous...\n",
       "7      Nzola and Brent have known each other for thre...\n",
       "8      Reza has known Dr. Shana Kelley for 1½ years a...\n",
       "9      The team came together just around 2 year ago ...\n",
       "10     The three of us have been working together for...\n",
       "11     Although we were both at Western, Mallorie and...\n",
       "12     Yuri and Alex have been working together for 7...\n",
       "13     Gil and Eyal are brothers, grew up together (3...\n",
       "14     Muneer, Andrei and Alex have known each other ...\n",
       "15     Jacalyn and J. Grant Miller, her husband, have...\n",
       "16     Ari, Jik and Rohan met early in the summer of ...\n",
       "17     Brandon and Mike met in October 2012 through F...\n",
       "18     Since starting my MBA at Rotman I have been tr...\n",
       "19     The founders know each other very well:\\r\\rEdm...\n",
       "20     Kalin and Alex have known each other since und...\n",
       "21     Dr Ozersky has known our advisory board for ov...\n",
       "22     We have known each other for many years. Rusla...\n",
       "23     This question does not apply since I am the on...\n",
       "24     The team started as a Capstone project with 4 ...\n",
       "25     Founders are alumni of Indian Institute of Tec...\n",
       "26           This team is newly formed for this project.\n",
       "27     Our management team has worked together previo...\n",
       "28                                        Doesn't apply.\n",
       "29     The management team, Mueller, Harb and Dwyer, ...\n",
       "                             ...                        \n",
       "353    Peter and I have known each other since I’ve m...\n",
       "354    Anthony and Christina met in school and have k...\n",
       "355    Sebastian, Thomas, and Yassir met at the Unive...\n",
       "356    After one year of working on our original N36 ...\n",
       "357    Chad Molleken and Aaliyah Ansari met in the su...\n",
       "358    The founders have known each other for several...\n",
       "359    I am currently the sole founder and actively s...\n",
       "360    The Founders have known each other for the las...\n",
       "361    Il Gon, Shirley, and Rui officially came on bo...\n",
       "362    The cofounders/brothers have known eachother f...\n",
       "363    Team members have been working with each other...\n",
       "364    As a team we have known each other collectivel...\n",
       "365    Co-Founders - ~1.5 years. Met at a conference ...\n",
       "366    Yao-Hong (me) and Shang knew each other during...\n",
       "367    Tharshi, Yara and Vishnu have known each other...\n",
       "368    I have known Richard for more than 4 years, he...\n",
       "369    The team know each other in various capacities...\n",
       "370    We have known each other for more than 7 Years...\n",
       "371    James and Ryan are working on their second sta...\n",
       "372    We've known each other for 6 years, and we met...\n",
       "373    4 years - as graduate students under the same ...\n",
       "374    Knew each other in highschool through the rave...\n",
       "375    I have been leading this resaerch project sinc...\n",
       "376    Nathan and Sergei had met in April of 2015 at ...\n",
       "377    Amy and Ji Duk were introduced to each other b...\n",
       "378         Marie Chevrier is a sole founder of Sampler.\n",
       "379    Prof. Poupart and Pablo met in May of this yea...\n",
       "380    The founders have known each other for 7 years...\n",
       "381    Maria is completing her MSc under Frank’s supe...\n",
       "382    The founders have known each other and have be...\n",
       "Name: Team Relationships, Length: 383, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"Team Relationships\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split by Whitespace and Remove Punctuation\n",
    "We may want the words, but without the punctuation like commas and quotes. We also want to keep contractions together.\n",
    "One way would be to split the document into words by white space (as in “2. Split by Whitespace“), then use string translation to replace all punctuation with nothing (e.g. remove it). Python provides a constant called string.punctuation that provides a great list of punctuation characters. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the function maketrans() to create a mapping table. We can create an empty mapping table, but the third argument of this function allows us to list all of the characters to remove during the translation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Although', 'there', 'arent', 'cofounders', 'in', 'strict', 'sense', 'one', 'of', 'our', 'key', 'employees', 'was', 'Jonathans', 'roommate', 'while', 'he', 'was', 'a', 'student', 'on', 'exchange', 'in', 'Helsinki', 'They', 'have', 'known', 'each', 'other', 'for', '12', 'years', 'Jonathan', 'has', 'successful', 'working', 'relationships', 'with', 'both', 'advisors', 'as', 'well', 'Dr', 'Katzman', 'will', 'be', 'the', 'principal', 'investigator', 'on', 'an', 'upcoming', 'clinical', 'trial', 'of', 'the', '1Datapoint', 'technology', 'Dr', 'Srigley', 'worked', 'with', '1Datapoint', 'on', 'the', 'project', 'that', 'has', 'been', 'published', 'in', 'IEEE']\n"
     ]
    }
   ],
   "source": [
    "table = str.maketrans('', '', string.punctuation)\n",
    "sample = dataset[\"Team Relationships\"][0]\n",
    "words = sample.split()\n",
    "stripped = [w.translate(table) for w in words]\n",
    "print(stripped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contractions like “aren’t” have become “arent” but “co-founders” has become “cofounders“."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is common to convert all words to one case.\n",
    "\n",
    "This means that the vocabulary will shrink in size, but some distinctions are lost (e.g. “Apple” the company vs “apple” the fruit is a commonly used example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['although', 'there', \"aren't\", 'co-founders', 'in', 'strict', 'sense,', 'one', 'of', 'our', 'key', 'employees', 'was', \"jonathan's\", 'roommate', 'while', 'he', 'was', 'a', 'student', 'on', 'exchange', 'in', 'helsinki.', 'they', 'have', 'known', 'each', 'other', 'for', '12', 'years.', 'jonathan', 'has', 'successful', 'working', 'relationships', 'with', 'both', 'advisors', 'as', 'well.', 'dr.', 'katzman', 'will', 'be', 'the', 'principal', 'investigator', 'on', 'an', 'upcoming', 'clinical', 'trial', 'of', 'the', '1datapoint', 'technology.', 'dr.', 'srigley', 'worked', 'with', '1datapoint', 'on', 'the', 'project', 'that', 'has', 'been', 'published', 'in', 'ieee.']\n"
     ]
    }
   ],
   "source": [
    "normalized_words = [word.lower() for word in words]\n",
    "print(normalized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and Cleaning with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Although there aren't co-founders in strict sense, one of our key employees was Jonathan's roommate while he was a student on exchange in Helsinki.\",\n",
       " 'They have known each other for 12 years.',\n",
       " 'Jonathan has successful working relationships with both advisors as well.',\n",
       " 'Dr. Katzman will be the principal investigator on an upcoming clinical trial of the 1Datapoint technology.',\n",
       " 'Dr. Srigley worked with 1Datapoint on the project that has been published in IEEE.']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split into sentences\n",
    "from nltk import sent_tokenize \n",
    "sentences = sent_tokenize(sample)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Although', 'there', 'are', \"n't\", 'co-founders', 'in', 'strict', 'sense', ',', 'one', 'of', 'our', 'key', 'employees', 'was', 'Jonathan', \"'s\", 'roommate', 'while', 'he', 'was', 'a', 'student', 'on', 'exchange', 'in', 'Helsinki', '.', 'They', 'have', 'known', 'each', 'other', 'for', '12', 'years', '.', 'Jonathan', 'has', 'successful', 'working', 'relationships', 'with', 'both', 'advisors', 'as', 'well', '.', 'Dr.', 'Katzman', 'will', 'be', 'the', 'principal', 'investigator', 'on', 'an', 'upcoming', 'clinical', 'trial', 'of', 'the', '1Datapoint', 'technology', '.', 'Dr.', 'Srigley', 'worked', 'with', '1Datapoint', 'on', 'the', 'project', 'that', 'has', 'been', 'published', 'in', 'IEEE', '.']\n"
     ]
    }
   ],
   "source": [
    "# Split into Words (It splits tokens based on white space and punctuation. For example, commas and periods are taken as separate tokens. Contractions are split apart (e.g. “What’s” becomes “What” “‘s“). Quotes are kept, and so on.)\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(sample)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Out Punctuation\n",
    "We can filter out all tokens that we are not interested in, such as all standalone punctuation.\n",
    "\n",
    "This can be done by iterating over all tokens and only keeping those tokens that are all alphabetic. Python has the function isalpha() that can be used. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Although', 'there', 'are', 'in', 'strict', 'sense', 'one', 'of', 'our', 'key', 'employees', 'was', 'Jonathan', 'roommate', 'while', 'he', 'was', 'a', 'student', 'on', 'exchange', 'in', 'Helsinki', 'They', 'have', 'known', 'each', 'other', 'for', 'years', 'Jonathan', 'has', 'successful', 'working', 'relationships', 'with', 'both', 'advisors', 'as', 'well', 'Katzman', 'will', 'be', 'the', 'principal', 'investigator', 'on', 'an', 'upcoming', 'clinical', 'trial', 'of', 'the', 'technology', 'Srigley', 'worked', 'with', 'on', 'the', 'project', 'that', 'has', 'been', 'published', 'in', 'IEEE']\n"
     ]
    }
   ],
   "source": [
    "# split into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(sample)\n",
    "# remove all tokens that are not alphabetic\n",
    "words = [word for word in tokens if word.isalpha()]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out Stop Words (and Pipeline)\n",
    "Stop words are those words that do not contribute to the deeper meaning of the phrase.\n",
    "\n",
    "They are the most common words such as: “the“, “a“, and “is“.\n",
    "\n",
    "For some applications like documentation classification, it may make sense to remove stop words.\n",
    "\n",
    "NLTK provides a list of commonly agreed upon stop words for a variety of languages, such as English. They can be loaded as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that they are all lower case and have punctuation removed.\n",
    "\n",
    "You could compare your tokens to the stop words and filter them out, but you must ensure that your text is prepared the same way.\n",
    "\n",
    "Let’s demonstrate this with a small pipeline of text preparation including:\n",
    "\n",
    "1. Load the raw text.\n",
    "2. Split into tokens.\n",
    "3. Convert to lowercase.\n",
    "4. Remove punctuation from each token.\n",
    "5. Filter out remaining tokens that are not alphabetic.\n",
    "6. Filter out tokens that are stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['although', 'nt', 'cofounders', 'strict', 'sense', 'one', 'key', 'employees', 'jonathan', 'roommate', 'student', 'exchange', 'helsinki', 'known', 'years', 'jonathan', 'successful', 'working', 'relationships', 'advisors', 'well', 'dr', 'katzman', 'principal', 'investigator', 'upcoming', 'clinical', 'trial', 'technology', 'dr', 'srigley', 'worked', 'project', 'published', 'ieee']\n"
     ]
    }
   ],
   "source": [
    "# split into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(sample)\n",
    "# convert to lower case\n",
    "tokens = [w.lower() for w in tokens]\n",
    "# remove punctuation from each word\n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in tokens]\n",
    "# remove remaining tokens that are not alphabetic\n",
    "words = [word for word in stripped if word.isalpha()]\n",
    "# filter out stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words = [w for w in words if not w in stop_words]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this example, we can see that in addition to all of the other transforms, stop words like “a” and “to” have been removed.\n",
    "\n",
    "We note that we are still left with tokens like “nt“."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stem words\n",
    "\n",
    "Stemming refers to the process of reducing each word to its root or base. For example “fishing,” “fished,” “fisher” all reduce to the stem “fish.” Some applications, like document classification, may benefit from stemming in order to both reduce the vocabulary and to focus on the sense or sentiment of a document rather than deeper meaning.\n",
    "\n",
    "There are many stemming algorithms, although a popular and long-standing method is the Porter Stemming algorithm. This method is available in NLTK via the PorterStemmer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['although', 'there', 'are', \"n't\", 'co-found', 'in', 'strict', 'sens', ',', 'one', 'of', 'our', 'key', 'employe', 'wa', 'jonathan', \"'s\", 'roommat', 'while', 'he', 'wa', 'a', 'student', 'on', 'exchang', 'in', 'helsinki', '.', 'they', 'have', 'known', 'each', 'other', 'for', '12', 'year', '.', 'jonathan', 'ha', 'success', 'work', 'relationship', 'with', 'both', 'advisor', 'as', 'well', '.', 'dr.', 'katzman', 'will', 'be', 'the', 'princip', 'investig', 'on', 'an', 'upcom', 'clinic', 'trial', 'of', 'the', '1datapoint', 'technolog', '.', 'dr.', 'srigley', 'work', 'with', '1datapoint', 'on', 'the', 'project', 'that', 'ha', 'been', 'publish', 'in', 'ieee', '.']\n"
     ]
    }
   ],
   "source": [
    "# split into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(sample)\n",
    "# stemming of words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "stemmed = [porter.stem(word) for word in tokens]\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
